# Reading2023

## I. Overview 

Total number of papers = 7

### 1.1 Per Area
* Computer vision = 5
* Natural Language Processing = 1
* Autonomous Driving 
* Deep Learning = 1

### 1.2 Per Year 
* 2023 = 3
* 2022 = 1
* 2021 = 1
* 2020 = 1
* 2019 = 1

## II. Publications
### 2.1 CVPR = 1
* [NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks](https://github.com/fawazsammani/nlxgpt) (2022) (3)
    * CV/NLP, text-image, model of (predict + explain). Disadvantage: prediction performance decareses.  

### 2.2 ICCV = 0 

### 2.3 ECCV = 0

### 2.4 NIPS = 1
* [Stand-Alone Self-Attention in Vision Models](https://proceedings.neurips.cc/paper/2019/file/3416a75f4cea9109507cacd8e2f2aefc-Paper.pdf) (2019) (4)
    * CV, self-attention stand-alone > CNN

### 2.5 ICML = 1
* [Generative Pretraining from Pixels](https://openai.com/research/image-gpt) (2020) (4)
    * CV, Image-GPT, Transformer at pixel-level for Image Classification

### 2.6 ICLR = 1
* [AN IMAGE IS WORTH 16X16 WORDS TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://openreview.net/forum?id=YicbFdNTTy) (2021) (5)
    * CV, ViT, Transformer at patch-level for Image Classification 
    
### 2.7 arXiv = 3
* [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) (2023) (5)
    * NLP, smaller LLM but better performace compared to chatGPT, modifications in details for efficiecy and smaller memory
* [Performance is not enough: a story of the Rashomon's quartet](https://arxiv.org/abs/2302.13356) (2023) (1)
    * DL, Visualization of ML models shows different models achieve the same result on same dataset, but each has different weights on different parameters in calculation. 
* [Segment Anything](https://arxiv.org/pdf/2304.02643.pdf)  (2023)  (5)
    * CV, Large-scaled Vision Model for segmentation, prompt-based, zero-shot
